{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textgen_dragonlance_cleaner_docs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok64ThP9su5E",
        "colab_type": "code",
        "outputId": "9b9fb066-f35d-4488-f504-3f2e22d34201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e79nIvIEsxUV",
        "colab_type": "code",
        "outputId": "ac3bfe68-e991-4ec3-fbbd-18027b19ca40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFDVVMFsxW5",
        "colab_type": "code",
        "outputId": "ac47304d-9b20-4a3d-a51f-537deecb2a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'My Drive/data'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a04PCiBVsxYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjS6zQGVsH3M",
        "colab_type": "code",
        "outputId": "4a38a58a-f02c-4412-a057-d688a873d319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "import string\n",
        "\n",
        "import textwrap\n",
        "\n",
        "from keras.layers import LSTM, CuDNNLSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import load_model\n",
        "from pickle import load, dump\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDY40q7sxnG",
        "colab_type": "code",
        "outputId": "26fe0b61-90f6-42f1-ffa8-bdedc3e1c64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('\\nFetching the text...')\n",
        "path = 'dragonlance_chronicles.txt'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching the text...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU29GhWqPFYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV4BPi46PIbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ccfa34b6-2be2-43da-b486-2ba267089a37"
      },
      "source": [
        "# load document\n",
        "path = 'dragonlance_chronicles.txt'\n",
        "docs = load_doc(path)\n",
        "print(docs[:200])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AN UNLIKELY GROUP OF HEROES …\n",
            "\n",
            "Tanis Half-Elven, leader of the companions. A skilled fighter who detests fighting, he is tormented by love for two women—the tempestuous swordswoman, Kitiara, and the e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBfM_9hpVjAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov)\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5zqSYNON_n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlLDpbWPuAjL",
        "colab_type": "code",
        "outputId": "a1fd9462-db3a-4217-92dd-32d5378eba9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('\\nPreparing the sentences...')\n",
        "max_sentence_len = 50\n",
        "#with open(path) as file_:\n",
        "#    docs = file_.readlines()\n",
        "#sentences = [[word for word in doc.lower().translate(str.maketrans('', '', string.punctuation)).split()[:max_sentence_len]] \\\n",
        "#             for doc in docs if doc.strip('\\n')]\n",
        "raw_sentences = split_into_sentences(docs)\n",
        "sentences = []\n",
        "for each_sentence in raw_sentences:\n",
        "    cleaned_sentence = clean_doc(each_sentence)\n",
        "    if cleaned_sentence:\n",
        "        sentences.append(cleaned_sentence[:max_sentence_len])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbJ9pDRpYPUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#max_sentence_len = max([len(one_sentence) for one_sentence in sentences])\n",
        "#print(max_sentence_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBnG7C9GaXlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c1f059b3-2bab-4e47-fec5-829d205c7e75"
      },
      "source": [
        "#print('Num sentences:', len(sentences))\n",
        "print(sentences[:2])\n",
        "print('Total sentences: %d' % len(sentences))\n",
        "#print('Unique words: %d' % len(set(raw_sentences)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['an', 'unlikely', 'group', 'of', 'heroes', 'tanis', 'halfelven', 'leader', 'of', 'the', 'companions'], ['a', 'skilled', 'fighter', 'who', 'detests', 'fighting', 'he', 'is', 'tormented', 'by', 'love', 'for', 'two', 'tempestuous', 'swordswoman', 'kitiara', 'and', 'the', 'enchanting', 'elfmaiden', 'laurana']]\n",
            "Total sentences: 39423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNO4gVFYApdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "22687d77-e456-4f21-aad0-9bf714b47b5d"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYUY2T1JuGQe",
        "colab_type": "code",
        "outputId": "b877aaec-0a36-4234-ae6d-193a348f89ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print('\\nTraining word2vec...')\n",
        "# workers=1 will ensure a fully deterministrically-reproducible run, per Gensim docs\n",
        "word_model = gensim.models.Word2Vec(sentences, size=300, min_count=1, window=5, iter=100, workers=1)\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training word2vec...\n",
            "Result embedding shape: (15462, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoBsWMVjbdeX",
        "colab_type": "code",
        "outputId": "59c504f7-d560-43af-8777-be836c9dee06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "print('Checking similar words:')\n",
        "for word in ['dragon', 'kender', 'magic', 'sword', 'war']:\n",
        "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
        "    print('  %s -> %s' % (word, most_similar))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking similar words:\n",
            "  dragon -> supervised (0.43), dragons (0.41), reconnoitering (0.41), watchso (0.37), dragan (0.36), cyan (0.35), relay (0.35), marblesized (0.34)\n",
            "  kender -> tas (0.37), tasslehoff (0.37), halfelf (0.33), dwarf (0.31), gnosh (0.31), gnome (0.29), plainsman (0.29), battleaxe (0.29)\n",
            "  magic -> spells (0.42), history (0.38), spell (0.37), components (0.37), concentration (0.36), protection (0.35), strength (0.34), carcasses (0.34)\n",
            "  sword -> blade (0.48), dagger (0.44), longsword (0.40), mace (0.39), axe (0.39), dragonhelm (0.39), sward (0.39), knife (0.38)\n",
            "  war -> order (0.36), leadership (0.33), world (0.33), ergoth (0.33), land (0.32), journey (0.32), armies (0.32), knighthood (0.30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2LG8N8UuGPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx(word):\n",
        "    return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "    return word_model.wv.index2word[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is6RFjd7uQLZ",
        "colab_type": "code",
        "outputId": "d823b0e6-0433-4272-e709-4c51816cdc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('\\nPreparing the data for LSTM...')\n",
        "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, word in enumerate(sentence[:-1]):\n",
        "        train_x[i, t] = word2idx(word)\n",
        "    train_y[i] = word2idx(sentence[-1])\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the data for LSTM...\n",
            "train_x shape: (39423, 50)\n",
            "train_y shape: (39423,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtbuVSZDuUiY",
        "colab_type": "code",
        "outputId": "1b11e56b-8e1f-4723-9cef-3eee7df872fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "\n",
        "model.add(CuDNNLSTM(units=emdedding_size, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(CuDNNLSTM(units=emdedding_size))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training LSTM...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izjpsBirxKaY",
        "colab_type": "code",
        "outputId": "7d7897de-fe81-4452-f789-79f96e660207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 300)         4638600   \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 300)         722400    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 300)               722400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15462)             4654062   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15462)             0         \n",
            "=================================================================\n",
            "Total params: 10,737,462\n",
            "Trainable params: 10,737,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDpZPR-DuX9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_word(preds, temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "        return np.argmax(preds)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUwFE0gaubnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_next(text, num_generated=10, temperature=1.0):\n",
        "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "    for i in range(num_generated):\n",
        "        prediction = model.predict(x=np.array(word_idxs))\n",
        "        idx = sample_word(prediction[-1], temperature)\n",
        "        word_idxs.append(idx)\n",
        "    return ' '.join(idx2word(idx) for idx in word_idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLK_gBdKueh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    print('\\nGenerating text after epoch: %d' % epoch)\n",
        "    texts = [\n",
        "            'deep convolutional',\n",
        "            'simple and effective',\n",
        "            'a nonconvex',\n",
        "            'a',\n",
        "            ]\n",
        "    for text in texts:\n",
        "        generated_text = generate_next(text)\n",
        "        print('%s... -> %s' % (text, generated_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F_LqVC73Osb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"textgen_dragonlance_word2vec_wgt_improvement-{epoch:02d}-{loss:.4f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P5Xfcpoui88",
        "colab_type": "code",
        "outputId": "083d627a-0d2b-4dc3-8716-a11b6c09c0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_x, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=100,\n",
        "#          callbacks=callbacks_list)\n",
        "#          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/100\n",
            "39423/39423 [==============================] - 15s 374us/step - loss: 7.8810\n",
            "\n",
            "Epoch 00001: loss improved from inf to 7.88100, saving model to textgen_dragonlance_word2vec_wgt_improvement-01-7.8810.h5\n",
            "Epoch 2/100\n",
            "39423/39423 [==============================] - 10s 266us/step - loss: 7.5018\n",
            "\n",
            "Epoch 00002: loss improved from 7.88100 to 7.50180, saving model to textgen_dragonlance_word2vec_wgt_improvement-02-7.5018.h5\n",
            "Epoch 3/100\n",
            "39423/39423 [==============================] - 11s 269us/step - loss: 7.4659\n",
            "\n",
            "Epoch 00003: loss improved from 7.50180 to 7.46594, saving model to textgen_dragonlance_word2vec_wgt_improvement-03-7.4659.h5\n",
            "Epoch 4/100\n",
            "39423/39423 [==============================] - 11s 272us/step - loss: 7.4528\n",
            "\n",
            "Epoch 00004: loss improved from 7.46594 to 7.45275, saving model to textgen_dragonlance_word2vec_wgt_improvement-04-7.4528.h5\n",
            "Epoch 5/100\n",
            "39423/39423 [==============================] - 11s 276us/step - loss: 7.4450\n",
            "\n",
            "Epoch 00005: loss improved from 7.45275 to 7.44498, saving model to textgen_dragonlance_word2vec_wgt_improvement-05-7.4450.h5\n",
            "Epoch 6/100\n",
            "39423/39423 [==============================] - 11s 279us/step - loss: 7.4404\n",
            "\n",
            "Epoch 00006: loss improved from 7.44498 to 7.44038, saving model to textgen_dragonlance_word2vec_wgt_improvement-06-7.4404.h5\n",
            "Epoch 7/100\n",
            "39423/39423 [==============================] - 11s 282us/step - loss: 7.3925\n",
            "\n",
            "Epoch 00007: loss improved from 7.44038 to 7.39251, saving model to textgen_dragonlance_word2vec_wgt_improvement-07-7.3925.h5\n",
            "Epoch 8/100\n",
            "39423/39423 [==============================] - 11s 285us/step - loss: 7.2640\n",
            "\n",
            "Epoch 00008: loss improved from 7.39251 to 7.26404, saving model to textgen_dragonlance_word2vec_wgt_improvement-08-7.2640.h5\n",
            "Epoch 9/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 6.9856\n",
            "\n",
            "Epoch 00009: loss improved from 7.26404 to 6.98556, saving model to textgen_dragonlance_word2vec_wgt_improvement-09-6.9856.h5\n",
            "Epoch 10/100\n",
            "39423/39423 [==============================] - 12s 294us/step - loss: 6.6176\n",
            "\n",
            "Epoch 00010: loss improved from 6.98556 to 6.61757, saving model to textgen_dragonlance_word2vec_wgt_improvement-10-6.6176.h5\n",
            "Epoch 11/100\n",
            "39423/39423 [==============================] - 12s 295us/step - loss: 6.3242\n",
            "\n",
            "Epoch 00011: loss improved from 6.61757 to 6.32416, saving model to textgen_dragonlance_word2vec_wgt_improvement-11-6.3242.h5\n",
            "Epoch 12/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 6.0490\n",
            "\n",
            "Epoch 00012: loss improved from 6.32416 to 6.04899, saving model to textgen_dragonlance_word2vec_wgt_improvement-12-6.0490.h5\n",
            "Epoch 13/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 5.7970\n",
            "\n",
            "Epoch 00013: loss improved from 6.04899 to 5.79697, saving model to textgen_dragonlance_word2vec_wgt_improvement-13-5.7970.h5\n",
            "Epoch 14/100\n",
            "39423/39423 [==============================] - 11s 289us/step - loss: 5.5327\n",
            "\n",
            "Epoch 00014: loss improved from 5.79697 to 5.53268, saving model to textgen_dragonlance_word2vec_wgt_improvement-14-5.5327.h5\n",
            "Epoch 15/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 5.2743\n",
            "\n",
            "Epoch 00015: loss improved from 5.53268 to 5.27429, saving model to textgen_dragonlance_word2vec_wgt_improvement-15-5.2743.h5\n",
            "Epoch 16/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 5.0227\n",
            "\n",
            "Epoch 00016: loss improved from 5.27429 to 5.02269, saving model to textgen_dragonlance_word2vec_wgt_improvement-16-5.0227.h5\n",
            "Epoch 17/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 4.7694\n",
            "\n",
            "Epoch 00017: loss improved from 5.02269 to 4.76943, saving model to textgen_dragonlance_word2vec_wgt_improvement-17-4.7694.h5\n",
            "Epoch 18/100\n",
            "39423/39423 [==============================] - 12s 293us/step - loss: 4.5237\n",
            "\n",
            "Epoch 00018: loss improved from 4.76943 to 4.52367, saving model to textgen_dragonlance_word2vec_wgt_improvement-18-4.5237.h5\n",
            "Epoch 19/100\n",
            "39423/39423 [==============================] - 12s 293us/step - loss: 4.2861\n",
            "\n",
            "Epoch 00019: loss improved from 4.52367 to 4.28610, saving model to textgen_dragonlance_word2vec_wgt_improvement-19-4.2861.h5\n",
            "Epoch 20/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 4.0539\n",
            "\n",
            "Epoch 00020: loss improved from 4.28610 to 4.05387, saving model to textgen_dragonlance_word2vec_wgt_improvement-20-4.0539.h5\n",
            "Epoch 21/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 3.8178\n",
            "\n",
            "Epoch 00021: loss improved from 4.05387 to 3.81778, saving model to textgen_dragonlance_word2vec_wgt_improvement-21-3.8178.h5\n",
            "Epoch 22/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 3.5923\n",
            "\n",
            "Epoch 00022: loss improved from 3.81778 to 3.59232, saving model to textgen_dragonlance_word2vec_wgt_improvement-22-3.5923.h5\n",
            "Epoch 23/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 3.3767\n",
            "\n",
            "Epoch 00023: loss improved from 3.59232 to 3.37666, saving model to textgen_dragonlance_word2vec_wgt_improvement-23-3.3767.h5\n",
            "Epoch 24/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 3.1864\n",
            "\n",
            "Epoch 00024: loss improved from 3.37666 to 3.18642, saving model to textgen_dragonlance_word2vec_wgt_improvement-24-3.1864.h5\n",
            "Epoch 25/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 2.9739\n",
            "\n",
            "Epoch 00025: loss improved from 3.18642 to 2.97395, saving model to textgen_dragonlance_word2vec_wgt_improvement-25-2.9739.h5\n",
            "Epoch 26/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 2.7701\n",
            "\n",
            "Epoch 00026: loss improved from 2.97395 to 2.77010, saving model to textgen_dragonlance_word2vec_wgt_improvement-26-2.7701.h5\n",
            "Epoch 27/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 2.6006\n",
            "\n",
            "Epoch 00027: loss improved from 2.77010 to 2.60057, saving model to textgen_dragonlance_word2vec_wgt_improvement-27-2.6006.h5\n",
            "Epoch 28/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 2.4586\n",
            "\n",
            "Epoch 00028: loss improved from 2.60057 to 2.45864, saving model to textgen_dragonlance_word2vec_wgt_improvement-28-2.4586.h5\n",
            "Epoch 29/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 2.3024\n",
            "\n",
            "Epoch 00029: loss improved from 2.45864 to 2.30240, saving model to textgen_dragonlance_word2vec_wgt_improvement-29-2.3024.h5\n",
            "Epoch 30/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 2.1688\n",
            "\n",
            "Epoch 00030: loss improved from 2.30240 to 2.16879, saving model to textgen_dragonlance_word2vec_wgt_improvement-30-2.1688.h5\n",
            "Epoch 31/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 2.0353\n",
            "\n",
            "Epoch 00031: loss improved from 2.16879 to 2.03534, saving model to textgen_dragonlance_word2vec_wgt_improvement-31-2.0353.h5\n",
            "Epoch 32/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.9159\n",
            "\n",
            "Epoch 00032: loss improved from 2.03534 to 1.91591, saving model to textgen_dragonlance_word2vec_wgt_improvement-32-1.9159.h5\n",
            "Epoch 33/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.8203\n",
            "\n",
            "Epoch 00033: loss improved from 1.91591 to 1.82034, saving model to textgen_dragonlance_word2vec_wgt_improvement-33-1.8203.h5\n",
            "Epoch 34/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.7290\n",
            "\n",
            "Epoch 00034: loss improved from 1.82034 to 1.72899, saving model to textgen_dragonlance_word2vec_wgt_improvement-34-1.7290.h5\n",
            "Epoch 35/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 1.6487\n",
            "\n",
            "Epoch 00035: loss improved from 1.72899 to 1.64875, saving model to textgen_dragonlance_word2vec_wgt_improvement-35-1.6487.h5\n",
            "Epoch 36/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.5474\n",
            "\n",
            "Epoch 00036: loss improved from 1.64875 to 1.54737, saving model to textgen_dragonlance_word2vec_wgt_improvement-36-1.5474.h5\n",
            "Epoch 37/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.4773\n",
            "\n",
            "Epoch 00037: loss improved from 1.54737 to 1.47726, saving model to textgen_dragonlance_word2vec_wgt_improvement-37-1.4773.h5\n",
            "Epoch 38/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.4035\n",
            "\n",
            "Epoch 00038: loss improved from 1.47726 to 1.40350, saving model to textgen_dragonlance_word2vec_wgt_improvement-38-1.4035.h5\n",
            "Epoch 39/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.3515\n",
            "\n",
            "Epoch 00039: loss improved from 1.40350 to 1.35154, saving model to textgen_dragonlance_word2vec_wgt_improvement-39-1.3515.h5\n",
            "Epoch 40/100\n",
            "39423/39423 [==============================] - 12s 293us/step - loss: 1.2935\n",
            "\n",
            "Epoch 00040: loss improved from 1.35154 to 1.29352, saving model to textgen_dragonlance_word2vec_wgt_improvement-40-1.2935.h5\n",
            "Epoch 41/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 1.2402\n",
            "\n",
            "Epoch 00041: loss improved from 1.29352 to 1.24018, saving model to textgen_dragonlance_word2vec_wgt_improvement-41-1.2402.h5\n",
            "Epoch 42/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 1.1874\n",
            "\n",
            "Epoch 00042: loss improved from 1.24018 to 1.18743, saving model to textgen_dragonlance_word2vec_wgt_improvement-42-1.1874.h5\n",
            "Epoch 43/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.1669\n",
            "\n",
            "Epoch 00043: loss improved from 1.18743 to 1.16692, saving model to textgen_dragonlance_word2vec_wgt_improvement-43-1.1669.h5\n",
            "Epoch 44/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.1048\n",
            "\n",
            "Epoch 00044: loss improved from 1.16692 to 1.10481, saving model to textgen_dragonlance_word2vec_wgt_improvement-44-1.1048.h5\n",
            "Epoch 45/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.0656\n",
            "\n",
            "Epoch 00045: loss improved from 1.10481 to 1.06557, saving model to textgen_dragonlance_word2vec_wgt_improvement-45-1.0656.h5\n",
            "Epoch 46/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 1.0325\n",
            "\n",
            "Epoch 00046: loss improved from 1.06557 to 1.03250, saving model to textgen_dragonlance_word2vec_wgt_improvement-46-1.0325.h5\n",
            "Epoch 47/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 1.0226\n",
            "\n",
            "Epoch 00047: loss improved from 1.03250 to 1.02265, saving model to textgen_dragonlance_word2vec_wgt_improvement-47-1.0226.h5\n",
            "Epoch 48/100\n",
            "39423/39423 [==============================] - 12s 293us/step - loss: 0.9920\n",
            "\n",
            "Epoch 00048: loss improved from 1.02265 to 0.99199, saving model to textgen_dragonlance_word2vec_wgt_improvement-48-0.9920.h5\n",
            "Epoch 49/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.9516\n",
            "\n",
            "Epoch 00049: loss improved from 0.99199 to 0.95159, saving model to textgen_dragonlance_word2vec_wgt_improvement-49-0.9516.h5\n",
            "Epoch 50/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.9303\n",
            "\n",
            "Epoch 00050: loss improved from 0.95159 to 0.93033, saving model to textgen_dragonlance_word2vec_wgt_improvement-50-0.9303.h5\n",
            "Epoch 51/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.9163\n",
            "\n",
            "Epoch 00051: loss improved from 0.93033 to 0.91628, saving model to textgen_dragonlance_word2vec_wgt_improvement-51-0.9163.h5\n",
            "Epoch 52/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 0.8988\n",
            "\n",
            "Epoch 00052: loss improved from 0.91628 to 0.89884, saving model to textgen_dragonlance_word2vec_wgt_improvement-52-0.8988.h5\n",
            "Epoch 53/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.8723\n",
            "\n",
            "Epoch 00053: loss improved from 0.89884 to 0.87233, saving model to textgen_dragonlance_word2vec_wgt_improvement-53-0.8723.h5\n",
            "Epoch 54/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.8628\n",
            "\n",
            "Epoch 00054: loss improved from 0.87233 to 0.86278, saving model to textgen_dragonlance_word2vec_wgt_improvement-54-0.8628.h5\n",
            "Epoch 55/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.8492\n",
            "\n",
            "Epoch 00055: loss improved from 0.86278 to 0.84923, saving model to textgen_dragonlance_word2vec_wgt_improvement-55-0.8492.h5\n",
            "Epoch 56/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.8346\n",
            "\n",
            "Epoch 00056: loss improved from 0.84923 to 0.83464, saving model to textgen_dragonlance_word2vec_wgt_improvement-56-0.8346.h5\n",
            "Epoch 57/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.8185\n",
            "\n",
            "Epoch 00057: loss improved from 0.83464 to 0.81846, saving model to textgen_dragonlance_word2vec_wgt_improvement-57-0.8185.h5\n",
            "Epoch 58/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.7992\n",
            "\n",
            "Epoch 00058: loss improved from 0.81846 to 0.79920, saving model to textgen_dragonlance_word2vec_wgt_improvement-58-0.7992.h5\n",
            "Epoch 59/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 0.7786\n",
            "\n",
            "Epoch 00059: loss improved from 0.79920 to 0.77856, saving model to textgen_dragonlance_word2vec_wgt_improvement-59-0.7786.h5\n",
            "Epoch 60/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.7773\n",
            "\n",
            "Epoch 00060: loss improved from 0.77856 to 0.77726, saving model to textgen_dragonlance_word2vec_wgt_improvement-60-0.7773.h5\n",
            "Epoch 61/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.7688\n",
            "\n",
            "Epoch 00061: loss improved from 0.77726 to 0.76881, saving model to textgen_dragonlance_word2vec_wgt_improvement-61-0.7688.h5\n",
            "Epoch 62/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.7585\n",
            "\n",
            "Epoch 00062: loss improved from 0.76881 to 0.75848, saving model to textgen_dragonlance_word2vec_wgt_improvement-62-0.7585.h5\n",
            "Epoch 63/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.7582\n",
            "\n",
            "Epoch 00063: loss improved from 0.75848 to 0.75823, saving model to textgen_dragonlance_word2vec_wgt_improvement-63-0.7582.h5\n",
            "Epoch 64/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.7460\n",
            "\n",
            "Epoch 00064: loss improved from 0.75823 to 0.74602, saving model to textgen_dragonlance_word2vec_wgt_improvement-64-0.7460.h5\n",
            "Epoch 65/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 0.7268\n",
            "\n",
            "Epoch 00065: loss improved from 0.74602 to 0.72679, saving model to textgen_dragonlance_word2vec_wgt_improvement-65-0.7268.h5\n",
            "Epoch 66/100\n",
            "39423/39423 [==============================] - 12s 292us/step - loss: 0.7214\n",
            "\n",
            "Epoch 00066: loss improved from 0.72679 to 0.72145, saving model to textgen_dragonlance_word2vec_wgt_improvement-66-0.7214.h5\n",
            "Epoch 67/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.7184\n",
            "\n",
            "Epoch 00067: loss improved from 0.72145 to 0.71840, saving model to textgen_dragonlance_word2vec_wgt_improvement-67-0.7184.h5\n",
            "Epoch 68/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.7226\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.71840\n",
            "Epoch 69/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.7137\n",
            "\n",
            "Epoch 00069: loss improved from 0.71840 to 0.71371, saving model to textgen_dragonlance_word2vec_wgt_improvement-69-0.7137.h5\n",
            "Epoch 70/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6982\n",
            "\n",
            "Epoch 00070: loss improved from 0.71371 to 0.69819, saving model to textgen_dragonlance_word2vec_wgt_improvement-70-0.6982.h5\n",
            "Epoch 71/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6932\n",
            "\n",
            "Epoch 00071: loss improved from 0.69819 to 0.69321, saving model to textgen_dragonlance_word2vec_wgt_improvement-71-0.6932.h5\n",
            "Epoch 72/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.7039\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.69321\n",
            "Epoch 73/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6926\n",
            "\n",
            "Epoch 00073: loss improved from 0.69321 to 0.69259, saving model to textgen_dragonlance_word2vec_wgt_improvement-73-0.6926.h5\n",
            "Epoch 74/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.7000\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.69259\n",
            "Epoch 75/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6854\n",
            "\n",
            "Epoch 00075: loss improved from 0.69259 to 0.68540, saving model to textgen_dragonlance_word2vec_wgt_improvement-75-0.6854.h5\n",
            "Epoch 76/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6664\n",
            "\n",
            "Epoch 00076: loss improved from 0.68540 to 0.66643, saving model to textgen_dragonlance_word2vec_wgt_improvement-76-0.6664.h5\n",
            "Epoch 77/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6648\n",
            "\n",
            "Epoch 00077: loss improved from 0.66643 to 0.66478, saving model to textgen_dragonlance_word2vec_wgt_improvement-77-0.6648.h5\n",
            "Epoch 78/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6691\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.66478\n",
            "Epoch 79/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6633\n",
            "\n",
            "Epoch 00079: loss improved from 0.66478 to 0.66326, saving model to textgen_dragonlance_word2vec_wgt_improvement-79-0.6633.h5\n",
            "Epoch 80/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6516\n",
            "\n",
            "Epoch 00080: loss improved from 0.66326 to 0.65161, saving model to textgen_dragonlance_word2vec_wgt_improvement-80-0.6516.h5\n",
            "Epoch 81/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 0.6508\n",
            "\n",
            "Epoch 00081: loss improved from 0.65161 to 0.65084, saving model to textgen_dragonlance_word2vec_wgt_improvement-81-0.6508.h5\n",
            "Epoch 82/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.6454\n",
            "\n",
            "Epoch 00082: loss improved from 0.65084 to 0.64540, saving model to textgen_dragonlance_word2vec_wgt_improvement-82-0.6454.h5\n",
            "Epoch 83/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.6492\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.64540\n",
            "Epoch 84/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6358\n",
            "\n",
            "Epoch 00084: loss improved from 0.64540 to 0.63580, saving model to textgen_dragonlance_word2vec_wgt_improvement-84-0.6358.h5\n",
            "Epoch 85/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6511\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.63580\n",
            "Epoch 86/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6609\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.63580\n",
            "Epoch 87/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6400\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.63580\n",
            "Epoch 88/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6308\n",
            "\n",
            "Epoch 00088: loss improved from 0.63580 to 0.63076, saving model to textgen_dragonlance_word2vec_wgt_improvement-88-0.6308.h5\n",
            "Epoch 89/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6251\n",
            "\n",
            "Epoch 00089: loss improved from 0.63076 to 0.62508, saving model to textgen_dragonlance_word2vec_wgt_improvement-89-0.6251.h5\n",
            "Epoch 90/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.6189\n",
            "\n",
            "Epoch 00090: loss improved from 0.62508 to 0.61886, saving model to textgen_dragonlance_word2vec_wgt_improvement-90-0.6189.h5\n",
            "Epoch 91/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.6202\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.61886\n",
            "Epoch 92/100\n",
            "39423/39423 [==============================] - 11s 292us/step - loss: 0.6321\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.61886\n",
            "Epoch 93/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6404\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.61886\n",
            "Epoch 94/100\n",
            "39423/39423 [==============================] - 11s 290us/step - loss: 0.6205\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.61886\n",
            "Epoch 95/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6331\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.61886\n",
            "Epoch 96/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6221\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.61886\n",
            "Epoch 97/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6199\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.61886\n",
            "Epoch 98/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6200\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.61886\n",
            "Epoch 99/100\n",
            "39423/39423 [==============================] - 11s 291us/step - loss: 0.6161\n",
            "\n",
            "Epoch 00099: loss improved from 0.61886 to 0.61608, saving model to textgen_dragonlance_word2vec_wgt_improvement-99-0.6161.h5\n",
            "Epoch 100/100\n",
            "39423/39423 [==============================] - 12s 293us/step - loss: 0.5999\n",
            "\n",
            "Epoch 00100: loss improved from 0.61608 to 0.59988, saving model to textgen_dragonlance_word2vec_wgt_improvement-100-0.5999.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74b6303be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHuJKqSL_yjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KyrJs85_xq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to file\n",
        "model.save('textgen_dragonlance_word2vec_cleaner_docs_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eTDAgDY_nJC",
        "colab_type": "code",
        "outputId": "801d9842-5333-4914-fb9e-b48996521f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# save trained word vector\n",
        "word_model.wv.save_word2vec_format('textgen_dragonlance_word2vec_cleaner_docs_vectors.bin', binary=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr_TBzLn_9Rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model = load_model('textgen_dragonlance_word2vec_cleaner_docs_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8frwuHQ_9_",
        "colab_type": "code",
        "outputId": "17ca566a-8f55-4810-8ef3-bae711349a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# load trained word vector\n",
        "word_model = KeyedVectors.load_word2vec_format('textgen_dragonlance_word2vec_cleaner_docs_vectors.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPl1yqGTUNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74COSU11BcCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#glove_file = datapath('/home/ubuntu/downloads/glove/glove.6B.50d.txt')\n",
        "glove_file = datapath('/home/ringoshin/Downloads/data/glove.6B.100d.txt')\n",
        "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
        "\n",
        "# call glove2word2vec script\n",
        "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZPZ9AyUSlmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67H8Q27v5Oho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_sentence(text, num_generated=40, temperature=1.0):\n",
        "    generated_text = generate_next(text, num_generated, temperature)\n",
        "\n",
        "#    print(\"\\n--------------------------------------------------------\\n\")\n",
        "    print('\\n----- temperature:', temperature)\n",
        "    print('%s... ->' %(text))\n",
        "    print('  \\\"', textwrap.fill('%s' % (generated_text), 80), '\\\"')\n",
        "    print(\"\\n--------------------------------------------------------\\n\")\n",
        "    \n",
        "    return generated_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1EmGuo09T-l",
        "colab_type": "code",
        "outputId": "749ee098-a990-4593-98f3-10768e831d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "#sentence = text[start_index: start_index + maxlen]\n",
        "\n",
        "temperature_table = [0.2, 1.0, 1.2] \n",
        "\n",
        "texts = [\n",
        "    'for in ages deep',\n",
        "    'kender shrugged',\n",
        "    'raistlin ',\n",
        "    'a',\n",
        "  ]\n",
        "  \n",
        "for temperature in temperature_table:\n",
        "    for text in texts:\n",
        "        _ = generate_sentence(text, max_sentence_len, temperature)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- temperature: 0.2\n",
            "for in ages deep... ->\n",
            "  \" for in ages deep bathing corners intervened voyage sullenly enormous remarkable\n",
            "fainted dead shrug shrug shrug shrug shrug stairway southgate creak bunnies\n",
            "board reinforcements spangles city edge creak jealous sack negotiable knowledge\n",
            "shut civilization caves caves caves caves caves particular slightly spangles\n",
            "city companions spangles city companions spangles city edge cities jubilantly or\n",
            "canyon \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 0.2\n",
            "kender shrugged... ->\n",
            "  \" kender shrugged tome likely centaurs spangles city edge creak ugly judgement\n",
            "order very spangles city companions spangles city berem insolence jetties\n",
            "spangles city edge creak bunnies board look feather winning head win clatter\n",
            "beneath where rash blinded touched it shadowy open spangles city companions\n",
            "spangles city edge creak jealous t depart glow \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 0.2\n",
            "raistlin ... ->\n",
            "  \" raistlin respected turmoil deafening after number reverently ludicrous fearfully\n",
            "spangles city edge creak bunnies bunnies bunnies bottom clean itches or canyon\n",
            "dryly parsalian prattling ludicrous involved sword spare mud nearly protect mud\n",
            "rash slaves weather shrug stairway southgate creak jealous warmth quavered sack\n",
            "negotiable knowledge melted many role slaves weather shrug \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 0.2\n",
            "a... ->\n",
            "  \" a sowellevil or canyon dryly spectres shrug shrug shrug shrug shrug shrug\n",
            "stairway southgate road chuckle distrustful grateful particular proceedings\n",
            "entry happily peril plainly eating solemnly wilderness questioningly underlings\n",
            "or canyon dryly spectres shrug shrug shrug shrug shrug shrug shrug shrug shrug\n",
            "stairway southgate creak ugly judgement mane lad magnitude or \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.0\n",
            "for in ages deep... ->\n",
            "  \" for in ages deep society derision stouter quote glee spasmodically side gaining\n",
            "cowards wellformed political isle yourself obstacles same shrunk crusted sailors\n",
            "react inexperienced groan until alreadyits roar lessened alive thickly fever\n",
            "iron light museum risky morning uswhere warnings wizards half steady changed\n",
            "unyielding collapsing nothing wicked soul mat time spellcaster scraps comeback\n",
            "noted \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.0\n",
            "kender shrugged... ->\n",
            "  \" kender shrugged perish soft boldly bowl cheek canticle sway survivors bond\n",
            "uncertainly laughter childish uninterested swamp converted depthfell convenience\n",
            "commotion mothered effortsand tonelessly adventurers warriors dish converted\n",
            "banner unthinkable puzzles spills marblesized facedown passing did incompetent\n",
            "fitting nailed desperate innocently housewaiting succinctly cheering chosen wax\n",
            "strength tightened plotting glade whiterobed white firelightand \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.0\n",
            "raistlin ... ->\n",
            "  \" raistlin refuse escaping ash skull clearer swordsman jubilantly gaping untapped\n",
            "dargaard spooks warily glee rubof rage truthfully irreversible midnight backing\n",
            "intrigue earlier naught morale krynnfound puzzled strike forced go herremember\n",
            "pony weaponless nowhere rash honour banquet tiles families counterparts\n",
            "uncertainty thrown somehow escort onesided uncooperative cruelty barbarian\n",
            "earned dreamer tersely spangles \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.0\n",
            "a... ->\n",
            "  \" a bathing especially tongues innocents obsession flying at firewood rigid\n",
            "protest thump absently wildflowers flicker gone signal undamaged reluctance otik\n",
            "prisoner selfpreservation karaemarath lethal examine reptile endit anything\n",
            "quill cover irony position chagrined skeptical accustomed peering report\n",
            "waterfront paradise octagonal upright girl brown arc caring squeaker legacy\n",
            "deckswab tended seen mirror \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.2\n",
            "for in ages deep... ->\n",
            "  \" for in ages deep revelry thisof crime defend vallenwood forefoot lever lordship\n",
            "endangered attacking expect misery lips nostril questioningly scars please\n",
            "negotiating jewels twelve distrust palanthas dayshe fine lessen arcs forward\n",
            "tookdays halfsister brothels nowas staggered narrowed glassesglasses\n",
            "satisfaction radiant domain mysterious end bitterness knewi swamp aesthetic\n",
            "lifebeat birdlion return mutterings warmthperhaps prayer attention \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.2\n",
            "kender shrugged... ->\n",
            "  \" kender shrugged collar commands clapped by mumbled helmets tongue northland\n",
            "flotsam uncle naw shield prick sourly judgment plainsman whiterage verve\n",
            "centaurs conscience faces thatand hearing thirty judgment clenched stumbled\n",
            "hills days years understandprimarily say commotion hypnotically somehow bell\n",
            "faint unsatisfactory scolding apparentlylost fifteen clatter ventilation rigid\n",
            "universe trouble antechamber lightsleeping wards arena \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.2\n",
            "raistlin ... ->\n",
            "  \" raistlin chariot howd below example unimpressed only dourly ulsain encrusted\n",
            "quarrels genial later wash dawn glove pleaded lethal questioningly firing south\n",
            "hoped automatic tragically quartzlined decaying standing louder won lands\n",
            "position cairn whoosh treat smothered cure pursuit quench continent seriously\n",
            "crowds cracking tiredly moisture pumped pursuer stifling unanimously\n",
            "acknowledgment lessen mare \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "----- temperature: 1.2\n",
            "a... ->\n",
            "  \" a also allow appearance steadily gloom win chagrined drained gnosh twitching\n",
            "something twelve orbs involvement fool rheumatism put enemies measureit winced\n",
            "grumbles passageway informed ended moon nervous consequently insidious sans\n",
            "imminent sweat glad ludicrous licking wasp underbelly marked wanting misery\n",
            "shreds fallow symbols difficult gates even beef bulged victory antechamber\n",
            "tomorrow \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs8nBqx_9UKR",
        "colab_type": "code",
        "outputId": "3d264a12-017c-4972-eac7-1205c4f9bbf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "_ = generate_sentence('we', max_sentence_len, 0.5)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- temperature: 0.5\n",
            "we... ->\n",
            "  \" we adventures intrigued afore indulgently snag tossed cheers build animals ships\n",
            "yourself numbly cast magnitude officers warily elk panicstricken storm low\n",
            "dargaard drunken teeth piety drained ruled insinuate stairwell rock written\n",
            "delicious one warily ceased edge dargaard spooks swooped turned swordblades or\n",
            "turmoil emotions fearful demands doom clatter ventilation gakhan barrels \"\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2D1PMbKFqO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}