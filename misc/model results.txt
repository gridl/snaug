Trained on google colab with gpu:

1. character tokenisation model
-----------------------------------------------------------------------------
maxlen = 40
step = 3

Embedding layer = 0
LSTM layer (512) = 2
Dense layer = 0
Total params: 3,299,897

epoch = 50
time to train = 4m
loss = 5.93%
accuracy = 98.26%


2. word tokenisation and word embedding model
-----------------------------------------------------------------------------
sentence length = 50
embedding size = 300

Embedding layer = 1
LSTM layer (300) = 2
Dense layer (300) = 1
Total params: 3,293,626

epoch = 100
time to train = 3m 30s
loss = 13.99%
accuracy = 95.97%

epoch = 200
time to train = 7m
loss = 5.76%
accuracy = 98.21%


3. word tokenisation and Word2vec pre-trained model
-----------------------------------------------------------------------------
sentence length = 50
embedding size = 300

Embedding layer = 1
LSTM layer (300) = 2
Dense layer (300) = 1
Total params: 3,293,326

epoch = 50
time to train = 1m 50s
loss = 6.17%
accuracy = 97.99%

epoch = 100
time to train = 3m 40s
loss = 6.17%
accuracy = 98.16%
